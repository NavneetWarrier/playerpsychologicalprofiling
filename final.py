# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wI2q3ZOMHUi309RUdWI4EH0USapqxDNE

**Application of ML in Psychological and Performance Profiling on Young Football Players to Predict Future Potential
**

21BRS1034 FASEEH MOHAMMED
21BRS1424 NAVNEET J WARRIER

DATA PREPROCESSING
"""

import pandas as pd


df = pd.read_csv('male_players.csv')


print(df.columns)


leagues_to_include = [
    "Major League Soccer",
    "La Liga",
    "La Liga 2",
    "Allsvenskan",
    "Super League",
    "Super Lig",
    "Liga Portugal",
    "Eredivisie",
    "Serie A",
    "Serie B",
    "Bundesliga",
    "2. Bundesliga",
    "Premier League",
    "Championship",
    "League One",
    "League Two",
    "Pro League",
    "Ligue 1",
    "Premier Division",
    "Liga Profesional"
]


filtered_df = df[df['league_name'].isin(leagues_to_include)]


filtered_df.to_csv('filtered_players.csv', index=False)


print(filtered_df.head())

filtered_df = pd.read_csv('filtered_players.csv')

# Filter to keep only players under the age of 22
young_players_df = filtered_df[filtered_df['age'] < 22]


young_players_df.to_csv('young_filtered_players.csv', index=False)

# Display the first few rows of the filtered DataFrame
print(young_players_df.head())

"""Twitter sentiment analysis using tweepy"""

import pandas as pd
import tweepy
from textblob import TextBlob

# Step 1: Set up Twitter API access
API_KEY = 'UtVyEvSX481THdHvySQyqt739'
API_SECRET_KEY = 'PuoE7q7E55E3QXaw6KfNFhUHfMHDC28HnQqmt350XHnPksw12B'
ACCESS_TOKEN = '1245585865637646336-BVVaHUTrJkTByVJWXXeozIMuvzLDlA'
ACCESS_TOKEN_SECRET = 'vGL1KVcR2Zt6wQqjQF6uU3mtRQSNLfkH6LzFgAUaftesW'
# Bearer Token
BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAHQtwgEAAAAA%2BGQQioA%2BqcpppiaigITqQttyPsU%3DZjHlD1Yt8beNeH4gfi7gYqCYmsSoxvstypoUB478YnbSol5pD1" # Replace with your actual Bearer token


# Authenticate to Twitter using API v2 Client
client = tweepy.Client(bearer_token=BEARER_TOKEN,
                       consumer_key=API_KEY,
                       consumer_secret=API_SECRET_KEY,
                       access_token=ACCESS_TOKEN,
                       access_token_secret=ACCESS_TOKEN_SECRET)

# Step 2: Load the player data
filtered_df = pd.read_csv('young_filtered_players.csv')

# Step 3: Fetch tweets and perform sentiment analysis
results = []

for index, row in filtered_df.iterrows():
    short_name = row['short_name']
    long_name = row['long_name']

    # Fetch tweets mentioning the player's name using search_recent_tweets
    tweets = client.search_recent_tweets(query=short_name, max_results=1) # Adjust max_results as needed

    # Check if tweets were found
    if tweets.data:
        for tweet in tweets.data:
            analysis = TextBlob(tweet.text)
            sentiment_score = analysis.sentiment.polarity

            results.append({
                'short_name': short_name,
                'long_name': long_name,
                'tweet': tweet.text,
                'sentiment_score': sentiment_score
            })

# Step 4: Create a DataFrame from the results
sentiment_df = pd.DataFrame(results)

# Step 5: Save the results to a CSV file
sentiment_df.to_csv('player_sentiment_analysis.csv', index=False)

# Display the first few rows of the sentiment DataFrame
print(sentiment_df.head())

"""Reddit sentiment analysis using praw"""

pip install praw textblob

import pandas as pd
import praw
from textblob import TextBlob
import warnings

# Step 0: Ignore warnings
warnings.filterwarnings("ignore")

# Step 1: Set up Reddit API access
reddit = praw.Reddit(
    client_id='auAfS6X2Lh_Fre5i94hdQA',
    client_secret='1VBrODVdZWdoMJ8YZbM1rGvo0aDuKg',
    user_agent='PoolaChips'
)

# Step 2: Load the player data
filtered_df = pd.read_csv('young_filtered_players.csv')

# Step 3: Fetch Reddit posts and perform sentiment analysis
results = []

# Print the available columns to check if 'short_name' exists
print(filtered_df.columns)

for index, row in filtered_df.iterrows():
    # Access the 'short_name' column using the correct name (case-sensitive)
    # If 'short_name' doesn't exist, replace with the actual column name
    short_name = row['short_name']
    long_name = row['long_name']

    # Fetch posts mentioning the player's name
    submissions = reddit.subreddit('all').search(short_name, limit=100)  # Adjust limit as needed

    for submission in submissions:
        analysis = TextBlob(submission.title + " " + submission.selftext)
        sentiment_score = analysis.sentiment.polarity  # Range from -1 (negative) to 1 (positive)

        results.append({
            'short_name': short_name,
            'long_name': long_name,
            'post_title': submission.title,
            'post_content': submission.selftext,
            'sentiment_score': sentiment_score
        })

# Step 4: Create a DataFrame from the results
sentiment_df = pd.DataFrame(results)

# Step 5: Save the results to a CSV file
sentiment_df.to_csv('player_reddit_sentiment_analysis.csv', index=False)

# Display the first few rows of the sentiment DataFrame
print(sentiment_df.head())

"""Google News Sentiment using NewsAPI"""

!pip install newsapi-python nltk tqdm

"""news from last 30 days , each player 5 articles = sentiment score"""

import pandas as pd
import numpy as np
from newsapi import NewsApiClient
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm import tqdm
import time
from datetime import datetime, timedelta

def analyze_player_potential_with_rate_limit(player_df, api_key, batch_size=20):
    """
    Analyze young football players' potential using sentiment analysis on news articles
    with proper rate limiting.

    Parameters:
    player_df (pandas.DataFrame): DataFrame containing player info with 'short_name' column
    api_key (str): Your NewsAPI key
    batch_size (int): Number of players to process before saving intermediate results

    Returns:
    pandas.DataFrame: Original dataframe with added sentiment scores and potential rating
    """
    # Download required NLTK data
    nltk.download('vader_lexicon')

    # Initialize sentiment analyzer
    sia = SentimentIntensityAnalyzer()

    # Initialize NewsAPI client
    newsapi = NewsApiClient(api_key=api_key)

    # Create lists to store results
    results = []

    # Calculate date range (last 30 days)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)

    # Process players in batches
    for i in range(0, len(player_df), batch_size):
        batch = player_df.iloc[i:i+batch_size]

        print(f"\nProcessing batch {i//batch_size + 1} of {len(player_df)//batch_size + 1}")

        for _, player in tqdm(batch.iterrows(), total=len(batch)):
            player_name = player['short_name']

            try:
                # Search for player news
                response = newsapi.get_everything(
                    q=f'"{player_name}" football player',
                    language='en',
                    from_param=start_date.strftime('%Y-%m-%d'),
                    to=end_date.strftime('%Y-%m-%d'),
                    sort_by='relevancy'
                )

                articles = response.get('articles', [])

                # Calculate sentiment scores
                sentiments = []
                for article in articles:
                    if article['description']:
                        sentiment = sia.polarity_scores(article['description'])
                        sentiments.append(sentiment['compound'])

                # Calculate metrics
                avg_sentiment = np.mean(sentiments) if sentiments else 0
                article_count = len(articles)

                results.append({
                    'short_name': player_name,
                    'sentiment_score': avg_sentiment,
                    'article_count': article_count,
                    'raw_sentiment_scores': sentiments
                })

                # Sleep to respect rate limits
                time.sleep(1.5)  # NewsAPI free tier allows 100 requests per day

            except Exception as e:
                print(f"\nError processing {player_name}: {str(e)}")
                results.append({
                    'short_name': player_name,
                    'sentiment_score': 0,
                    'article_count': 0,
                    'raw_sentiment_scores': []
                })

        # Save intermediate results
        intermediate_df = pd.DataFrame(results)
        intermediate_df.to_csv(f'intermediate_results_batch_{i//batch_size}.csv', index=False)
        print(f"\nSaved intermediate results for batch {i//batch_size + 1}")

        # Sleep between batches
        time.sleep(5)

    # Combine all results
    results_df = pd.DataFrame(results)

    # Merge with original dataframe
    final_df = player_df.merge(results_df, on='short_name', how='left')

    # Calculate normalized scores
    max_articles = final_df['article_count'].max()
    if max_articles > 0:
        final_df['coverage_score'] = (final_df['article_count'] / max_articles) * 100
    else:
        final_df['coverage_score'] = 0

    # Normalize sentiment scores to 0-100 scale
    sentiment_min = final_df['sentiment_score'].min()
    sentiment_max = final_df['sentiment_score'].max()
    if sentiment_max > sentiment_min:
        final_df['normalized_sentiment'] = (
            (final_df['sentiment_score'] - sentiment_min) /
            (sentiment_max - sentiment_min) * 100
        )
    else:
        final_df['normalized_sentiment'] = 50  # Default to neutral if all scores are the same

    # Calculate potential rating
    final_df['potential_rating'] = (
        final_df['normalized_sentiment'] * 0.7 +
        final_df['coverage_score'] * 0.3
    ).round(2)

    return final_df

def main():
    # You'll need to sign up for a free API key at newsapi.org
    API_KEY = '47b8ccbe77164348aecd32e34d39850d'  # Replace with your actual API key

    # Read your dataset
    players_df = pd.read_csv('usethis.csv') #update short names

    # Set batch size based on your needs (smaller batch size = more frequent saves)
    BATCH_SIZE = 30

    # Run analysis
    results_df = analyze_player_potential_with_rate_limit(
        players_df,
        API_KEY,
        batch_size=BATCH_SIZE
    )

    # Save final results
    results_df.to_csv('player_potential_analysis_final.csv', index=False)

    print("\nAnalysis complete! Results saved to 'player_potential_analysis_final.csv'")

if __name__ == "__main__":
    main()

"""Sentiment Analyse Twitter Dataset"""

import pandas as pd
# Load your datasets
players_df = pd.read_csv('young_filtered_players.csv')  # Update with your file path
tweets_df = pd.read_csv('fifa_2022_tweets.csv')    # Update with your file path

# Combine short_name and long_name into a list of names to search for
player_names = pd.Series(players_df['short_name'].tolist() + players_df['long_name'].tolist()).unique()

# Create a regex pattern to search for player names
pattern = '|'.join(player_names)

# Filter tweets that contain any of the player names
filtered_tweets = tweets_df[tweets_df['Tweet'].str.contains(pattern, case=False, na=False)]

# Save the filtered tweets to a new CSV file (optional)
filtered_tweets.to_csv('filtered_tweets.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt # Import the necessary module

df = pd.read_csv('filtered_tweets.csv')
df.Sentiment.value_counts().plot(kind = 'bar')
plt.title("Sentiment Value Count")
plt.show();

plt.figure(figsize=(14, 6))

df['Source of Tweet'].value_counts().head(10).plot(kind = 'bar')
plt.title("Top 10 Source of Tweet Count")
plt.show();

import pandas as pd
import matplotlib.pyplot as plt

time_interval = 'H'
df['Date Created'] = pd.to_datetime(df['Date Created'])
aggregated_df = df.groupby([pd.Grouper(key='Date Created', freq=time_interval), 'Sentiment']).size().unstack()

# Plot time series sentiment trends
plt.figure(figsize=(14, 6))
aggregated_df.plot(kind='line', marker='o')
plt.title('Time Series Sentiment Trends during FIFA World Cup 2022')
plt.xlabel('Time')
plt.ylabel('Number of Tweets')
plt.legend(title='Sentiment')
plt.grid()
plt.show();

"""Accuracy"""

from nltk.sentiment.vader import SentimentIntensityAnalyzer # Import SentimentIntensityAnalyzer from nltk.sentiment.vader
import nltk

nltk.download('vader_lexicon') # Download the lexicon for sentiment analysis

# Create a SentimentIntensityAnalyzer object
sia = SentimentIntensityAnalyzer()

""" will later be used to analyze the sentiment of texts by calling its polarity_scores method on input sentences.

"""

# Function to analyze sentiment and return sentiment label
def get_sentiment_label(text):
    sentiment_scores = sia.polarity_scores(text)

    if sentiment_scores['compound'] > 0.05:
        return "Positive"
    elif sentiment_scores['compound'] < -0.05:
        return "Negative"
    else:
        return "Neutral"

# Apply the sentiment analysis function to the 'Tweet' column # Assuming your original tweets are in 'Tweet' column
df['vader_sentiment'] = df['Tweet'].apply(get_sentiment_label)
#If the column containing the filtered tweets is named something other than 'Tweet',
# replace 'Tweet' with the actual column name.

"""This function takes text (a string) as input and:
Calculates sentiment scores using the VADER analyzer object (sia).
The polarity_scores() method returns a dictionary of scores (including compound).
Analyzes the compound score:
The compound score is a normalized value between -1 (most negative) to 1 (most positive).
If:
compound > 0.05: The text is Positive.
compound < -0.05: The text is Negative.
"""

df['Sentiment'] = df.Sentiment.str.title()

# Calculate the accuracy
correct_predictions = (df['vader_sentiment'] == df['Sentiment']).sum()
total_predictions = len(df)
accuracy = correct_predictions / total_predictions * 100

print("Accuracy: {:.2f}%".format(accuracy))

"""code evaluates the accuracy of VADER sentiment predictions by comparing them with the truth labels in the Sentiment column. calculates the accuracy as a percentage of correct predictions.

Distribution of vader  (analysis) :
"""

!pip install transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification # Import AutoTokenizer and AutoModelForSequenceClassification

# Load pre-trained RoBERTa model and tokenizer
model_name = "cardiffnlp/twitter-roberta-base-sentiment"  # try this
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

def polarity_scores_roberta(Input):
    # Tokenize the input text using the tokenizer
    encoded_text = tokenizer(Input, return_tensors='pt')
    # Pass the tokenized input to the model and get the output
    output = model(**encoded_text)
    # Extract the scores from the output and convert it to a NumPy array
    scores = output[0][0].detach().numpy()
    # Apply softmax on the scores to normalize them
    scores = softmax(scores)
    # Return the n

!pip install tqdm # Install the tqdm library if you haven't already
from tqdm import tqdm # Import the tqdm function
import pandas as pd # Assuming you are using pandas for df
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax

res = []
for i, row in tqdm(df.iterrows(), total=len(df)):
  text= row['Tweet']
  res.append(polarity_scores_roberta(text))

!pip install vaderSentiment # Install the vaderSentiment package
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # Import SentimentIntensityAnalyzer after installing the package

def get_sentiment(tweets):
    analyzer = SentimentIntensityAnalyzer()
    total_score = {'Positive': 0, 'Negative': 0}
    for tweet in tweets:
        score = analyzer.polarity_scores(tweet.text)['compound']
        if score >= .05:
            total_score['Positive'] += 1
        elif score <= -0.05:
            total_score['Negative'] += 1
    return (round((total_score['Positive']\
           /sum(total_score.values())) * 100, 2),
            round((total_score['Negative']\
           /sum(total_score.values())) * 100, 2))

!pip install transformers
!pip install tqdm
!pip install vaderSentiment

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
import pandas as pd
from scipy.special import softmax
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt
import seaborn as sns


# Load pre-trained RoBERTa model and tokenizer
model_name = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)


def polarity_scores_roberta(Input):
    # Tokenize the input text using the tokenizer
    encoded_text = tokenizer(Input, return_tensors='pt')
    # Pass the tokenized input to the model and get the output
    output = model(**encoded_text)
    # Extract the scores from the output and convert it to a NumPy array
    scores = output[0][0].detach().numpy()
    # Apply softmax on the scores to normalize them
    scores = softmax(scores)
    # Return the scores
    return scores  # Added return statement


def get_sentiment(tweets):
    analyzer = SentimentIntensityAnalyzer()
    total_score = {'Positive': 0, 'Negative': 0}
    for tweet in tweets:
        score = analyzer.polarity_scores(tweet.text)['compound']
        if score >= .05:
            total_score['Positive'] += 1
        elif score <= -0.05:
            total_score['Negative'] += 1
    return (round((total_score['Positive']\
           /sum(total_score.values())) * 100, 2),
            round((total_score['Negative']\
           /sum(total_score.values())) * 100, 2))

def get_vader_sentiment(text):
    """Calculates sentiment score using VADER."""
    analyzer = SentimentIntensityAnalyzer()
    score = analyzer.polarity_scores(text)['compound']
    if score >= 0.05:
        return "Positive"
    elif score <= -0.05:
        return "Negative"
    else:
        return "Neutral"

def generate_psychological_report(df):
    """Generates a psychological report based on player statistics."""

    report = ""

    # Player Statistics Summary
    report += "## Player Statistics Summary\n\n"
    # If the columns have different names, replace them here
    if 'overall' in df.columns:
        report += f"**Average Overall Rating:** {df['overall'].mean():.2f}\n"
    if 'potential' in df.columns:
        report += f"**Average Potential:** {df['potential'].mean():.2f}\n"
    if 'age' in df.columns:
        report += f"**Average Age:** {df['age'].mean():.2f}\n\n"

    # Sentiment Analysis Summary
    report += "## Sentiment Analysis Summary\n\n"
    report += "This section summarizes the sentiment analysis performed on tweets related to the top players. Sentiment scores are based on VADER and RoBERTa models.\n\n"

    # Assuming df contains 'vader_sentiment' column
    sentiment_counts = df['vader_sentiment'].value_counts()

    report += "**Vader Sentiment Distribution:**\n"
    report += str(sentiment_counts) + "\n\n"

    # Include plots if needed
    # Example using matplotlib:
    plt.figure(figsize=(8, 6))
    sns.countplot(x='vader_sentiment', data=df)
    plt.title('Distribution of Vader Sentiment')
    plt.savefig('vader_sentiment_distribution.png')
    report += "![](/content/vader_sentiment_distribution.png)\n\n"

    return report

# Load the dataframe
df = pd.read_csv("newtweets.csv")

# Check the existing column names in the DataFrame
print(df.columns)

# Replace 'tweet_text' with the actual column name containing the tweets
# For example, if the column is named 'Tweet', change the line below to:
df['vader_sentiment'] = df['Tweet'].apply(get_vader_sentiment)
tweet_column_name = 'Tweet'  # Replace 'Tweet' with the correct column name if necessary
df['vader_sentiment'] = df[tweet_column_name].apply(get_vader_sentiment)
sentiment_counts = df['vader_sentiment'].value_counts()

psychological_report = generate_psychological_report(df)
psychological_report

"""Sentiment & Media features with Technical/physical attributes"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import xgboost as xgb
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns

class PlayerPotentialPredictor:
    def __init__(self):
        self.rf_model = None
        self.xgb_model = None
        self.scaler = StandardScaler()
        self.feature_importance = None
        self.clusters = None
        self.pca = None

    def prepare_features(self, df):
        """
        Prepare features for machine learning models
        """
        # Technical attributes
        technical_cols = [
            'dribbling', 'passing', 'shooting', 'skill_ball_control',
            'attacking_crossing', 'attacking_finishing', 'skill_curve',
            'skill_fk_accuracy', 'skill_long_passing'
        ]

        # Physical attributes
        physical_cols = [
            'pace', 'physic', 'power_stamina', 'power_strength',
            'movement_acceleration', 'movement_sprint_speed', 'movement_agility',
            'movement_balance', 'height_cm', 'weight_kg'
        ]

        # Mental attributes
        mental_cols = [
            'mentality_vision', 'mentality_composure', 'mentality_positioning',
            'mentality_interceptions', 'mentality_aggression', 'weak_foot',
            'skill_moves', 'international_reputation', 'mentality_penalties'
        ]

        # Sentiment and media features
        sentiment_cols = ['normalized_sentiment', 'coverage_score', 'article_count']

        # Basic info
        basic_cols = ['age', 'overall', 'value_eur', 'wage_eur']

        # Combine all features
        feature_cols = technical_cols + physical_cols + mental_cols + sentiment_cols + basic_cols

        # Create feature matrix
        X = df[feature_cols].copy()

        # Handle missing values
        X = X.fillna(X.mean())

        # Scale features
        X_scaled = self.scaler.fit_transform(X)

        return X_scaled, feature_cols

    def train_models(self, df, target_col='potential'):
        """
        Train multiple ML models for player potential prediction
        """
        X_scaled, self.feature_cols = self.prepare_features(df)
        y = df[target_col]

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )

        # Train Random Forest
        self.rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=15,
            random_state=42
        )
        self.rf_model.fit(X_train, y_train)

        # Train XGBoost
        self.xgb_model = xgb.XGBRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=7,
            random_state=42
        )
        self.xgb_model.fit(X_train, y_train)

        # Calculate feature importance
        self.feature_importance = pd.DataFrame({
            'feature': self.feature_cols,
            'importance': self.rf_model.feature_importances_
        }).sort_values('importance', ascending=False)

        # Perform clustering
        self.clusters = KMeans(n_clusters=5, random_state=42).fit(X_scaled)

        # Dimensionality reduction for visualization
        self.pca = PCA(n_components=2)
        self.pca_result = self.pca.fit_transform(X_scaled)

        # Return model evaluation metrics
        return self.evaluate_models(X_test, y_test)

    def evaluate_models(self, X_test, y_test):
        """
        Evaluate model performance
        """
        results = {}

        # Random Forest evaluation
        rf_pred = self.rf_model.predict(X_test)
        results['rf'] = {
            'mse': mean_squared_error(y_test, rf_pred),
            'r2': r2_score(y_test, rf_pred),
            'cv_scores': cross_val_score(self.rf_model, X_test, y_test, cv=5)
        }

        # XGBoost evaluation
        xgb_pred = self.xgb_model.predict(X_test)
        results['xgb'] = {
            'mse': mean_squared_error(y_test, xgb_pred),
            'r2': r2_score(y_test, xgb_pred),
            'cv_scores': cross_val_score(self.xgb_model, X_test, y_test, cv=5)
        }

        return results

    def predict_potential(self, player_data):
        """
        Make predictions for new players
        """
        X_scaled, _ = self.prepare_features(player_data)

        # Ensemble prediction (average of both models)
        rf_pred = self.rf_model.predict(X_scaled)
        xgb_pred = self.xgb_model.predict(X_scaled)

        return (rf_pred + xgb_pred) / 2

    def visualize_results(self, df):
        """
        Create ML-specific visualizations
        """
        fig = plt.figure(figsize=(20, 15))
        gs = fig.add_gridspec(3, 2)

        # 1. Feature Importance Plot
        ax1 = fig.add_subplot(gs[0, 0])
        sns.barplot(
            data=self.feature_importance.head(15),
            x='importance',
            y='feature',
            ax=ax1
        )
        ax1.set_title('Top 15 Most Important Features')

        # 2. Cluster Visualization
        ax2 = fig.add_subplot(gs[0, 1])
        scatter = ax2.scatter(
            self.pca_result[:, 0],
            self.pca_result[:, 1],
            c=self.clusters.labels_,
            cmap='viridis'
        )
        ax2.set_title('Player Clusters (PCA)')
        plt.colorbar(scatter)

        # 3. Predicted vs Actual Plot
        ax3 = fig.add_subplot(gs[1, 0])
        X_scaled, _ = self.prepare_features(df)
        predictions = self.predict_potential(df)

        ax3.scatter(df['potential'], predictions, alpha=0.5)
        ax3.plot([40, 100], [40, 100], 'r--')  # Perfect prediction line
        ax3.set_xlabel('Actual Potential')
        ax3.set_ylabel('Predicted Potential')
        ax3.set_title('Predicted vs Actual Potential')

        # 4. Error Distribution
        ax4 = fig.add_subplot(gs[1, 1])
        errors = predictions - df['potential']
        sns.histplot(errors, ax=ax4)
        ax4.set_title('Prediction Error Distribution')

        # 5. Model Performance Comparison
        ax5 = fig.add_subplot(gs[2, :])
        X_scaled, _ = self.prepare_features(df)
        y = df['potential']
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

        results = self.evaluate_models(X_test, y_test)
        model_scores = pd.DataFrame({
            'Model': ['Random Forest', 'XGBoost'],
            'R² Score': [results['rf']['r2'], results['xgb']['r2']],
            'MSE': [results['rf']['mse'], results['xgb']['mse']]
        })

        sns.barplot(data=model_scores.melt(id_vars='Model'), x='Model', y='value', hue='variable', ax=ax5)
        ax5.set_title('Model Performance Comparison')

        plt.tight_layout()
        plt.show()

def main():
    # Load data
    df = pd.read_csv('player_potential_analysis_final.csv')

    # Initialize and train the model
    predictor = PlayerPotentialPredictor()
    results = predictor.train_models(df)

    # Visualize results
    predictor.visualize_results(df)

    # Print model performance
    print("\nModel Performance:")
    print("\nRandom Forest:")
    print(f"R² Score: {results['rf']['r2']:.3f}")
    print(f"MSE: {results['rf']['mse']:.3f}")
    print(f"Cross-validation scores: {results['rf']['cv_scores'].mean():.3f} (±{results['rf']['cv_scores'].std():.3f})")

    print("\nXGBoost:")
    print(f"R² Score: {results['xgb']['r2']:.3f}")
    print(f"MSE: {results['xgb']['mse']:.3f}")
    print(f"Cross-validation scores: {results['xgb']['cv_scores'].mean():.3f} (±{results['xgb']['cv_scores'].std():.3f})")

if __name__ == "__main__":
    main()

"""The scatter plot shows player clusters in a 2D space, with each point representing a player, and the color indicating the cluster they belong to.
Use Case: This helps teams identify patterns, like a cluster of highly skilled attackers or physically strong defenders.

Prediction error distrib : To analyze how well the model predicts player potential. The histogram shows the distribution of prediction errors (i.e., how far off the model’s predictions are from the true potential values

Per player Sentiment i.e 'Voice of the people' :
"""

import pandas as pd
import re

def generate_name_variations(short_name, long_name):
    """Generate all possible variations of a player's name including @ mentions"""
    names = set()

    # Split names
    short_parts = short_name.split()
    long_parts = long_name.split()

    # Add original forms
    names.add(short_name)
    names.add(long_name)

    # Add individual parts
    names.update(short_parts)
    names.update(long_parts)

    # Add @ versions
    at_names = {f"@{name}" for name in names}
    names.update(at_names)

    return list(names)

def analyze_player_sentiment(tweets_df, player_df):
    """Analyze sentiment for each player and return results"""
    results = []

    for _, player in player_df.iterrows():
        short_name = player['short_name']
        long_name = player['long_name']

        # Generate name variations
        name_variations = generate_name_variations(short_name, long_name)

        # Create regex pattern
        pattern = '|'.join(map(re.escape, name_variations))

        # Find tweets mentioning the player
        player_tweets = tweets_df[tweets_df['Tweet'].str.contains(pattern, case=False, regex=True)]

        if len(player_tweets) > 0:
            # Calculate sentiment percentages
            sentiment_counts = player_tweets['Sentiment'].value_counts()
            total_mentions = len(player_tweets)

            result = {
                'player': long_name,
                'positive_pct': (sentiment_counts.get('positive', 0) / total_mentions) * 100,
                'negative_pct': (sentiment_counts.get('negative', 0) / total_mentions) * 100,
                'neutral_pct': (sentiment_counts.get('neutral', 0) / total_mentions) * 100
            }

            results.append(result)

    return pd.DataFrame(results)

# Load the datasets
tweets_df = pd.read_csv('fifa_2022_tweets.csv')
players_df = pd.read_csv('usethis.csv')

# Run the analysis
results_df = analyze_player_sentiment(tweets_df, players_df)

# Print results for verification
print(results_df)

"""Overall Analysis :"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

def calculate_player_attributes(player_data):
    """
    Calculate player attributes based on sentiment analysis and FIFA stats
    """
    # Core metrics from sentiment analysis
    confidence = player_data['normalized_sentiment'] / 100
    media_presence = player_data['coverage_score'] / 100

    # Normalize FIFA attributes to 0-1 range
    technical = (player_data[['dribbling', 'passing', 'shooting', 'skill_ball_control']].mean()) / 100
    physical = (player_data[['pace', 'physic', 'power_stamina', 'power_strength']].mean()) / 100
    mental = (player_data[['mentality_vision', 'mentality_composure', 'mentality_positioning']].mean()) / 100

    # Calculate composite scores
    development_potential = (player_data['potential'] - player_data['overall']) / 25  # Normalize growth potential
    value_efficiency = np.log(player_data['value_eur']) / np.log(100000000)  # Logarithmic scale for transfer value

    return {
        'Technical': technical,
        'Physical': physical,
        'Mental': mental,
        'Media Impact': confidence * 0.7 + media_presence * 0.3,
        'Growth Potential': development_potential,
        'Value Efficiency': value_efficiency
    }

def create_player_profile_visualization(results_df, player_name=None, top_n=5, min_age=17, max_age=23):

    # Filter for young players
    young_players = results_df[
        (results_df['age'] >= min_age) &
        (results_df['age'] <= max_age)
    ].copy()

    if player_name is None:
        # Get top N young players by potential rating and FIFA potential
        players_to_analyze = young_players.nlargest(
            top_n,
            ['potential_rating', 'potential']
        )
    else:
        # Get specific player and top N-1 others
        player_data = young_players[young_players['short_name'] == player_name]
        others = young_players[young_players['short_name'] != player_name].nlargest(
            top_n-1,
            ['potential_rating', 'potential']
        )
        players_to_analyze = pd.concat([player_data, others])

    # Create subplot grid
    fig = plt.figure(figsize=(20, 15))
    gs = fig.add_gridspec(3, 2)

    # 1. Enhanced Radar Chart
    ax1 = fig.add_subplot(gs[0, 0], polar=True)

    attributes = ['Technical', 'Physical', 'Mental', 'Media Impact', 'Growth Potential', 'Value Efficiency']
    angles = [n / float(len(attributes)) * 2 * np.pi for n in range(len(attributes))]
    angles += angles[:1]

    for idx, (_, player) in enumerate(players_to_analyze.iterrows()):
        values = list(calculate_player_attributes(player).values())
        values += values[:1]

        color = plt.cm.viridis(idx/len(players_to_analyze))
        ax1.plot(angles, values, linewidth=2, label=f"{player['short_name']} ({player['age']})", color=color)
        ax1.fill(angles, values, alpha=0.25, color=color)

    ax1.set_xticks(angles[:-1])
    ax1.set_xticklabels(attributes)
    ax1.set_title('Player Complete Profile Analysis', pad=20)
    ax1.legend(loc='upper right', bbox_to_anchor=(0.1, 1.1))

    # 2. Potential vs Current Ability Matrix
    ax2 = fig.add_subplot(gs[0, 1])
    scatter = ax2.scatter(
        players_to_analyze['overall'],
        players_to_analyze['potential'],
        c=players_to_analyze['potential_rating'],
        s=100,
        cmap='viridis'
    )

    for _, player in players_to_analyze.iterrows():
        ax2.annotate(
            player['short_name'],
            (player['overall'], player['potential']),
            xytext=(5, 5),
            textcoords='offset points'
        )

    ax2.set_xlabel('Current Overall Rating')
    ax2.set_ylabel('Potential Rating')
    ax2.set_title('Current vs Potential Ability')
    plt.colorbar(scatter, label='Sentiment-based Potential Rating')

    # 3. Key Attributes Comparison
    ax3 = fig.add_subplot(gs[1, 0])
    key_stats = ['pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic']
    stats_data = players_to_analyze[key_stats].T
    stats_data.columns = players_to_analyze['short_name']

    sns.heatmap(
        stats_data,
        annot=True,
        cmap='YlOrRd',
        center=50,
        ax=ax3
    )
    ax3.set_title('Key Attributes Comparison')

    # 4. Value vs Potential Plot
    ax4 = fig.add_subplot(gs[1, 1])
    ax4.scatter(
        np.log10(players_to_analyze['value_eur']),
        players_to_analyze['potential_rating'],
        s=100
    )

    for _, player in players_to_analyze.iterrows():
        ax4.annotate(
            player['short_name'],
            (np.log10(player['value_eur']), player['potential_rating']),
            xytext=(5, 5),
            textcoords='offset points'
        )

    ax4.set_xlabel('Log10(Value in EUR)')
    ax4.set_ylabel('Potential Rating')
    ax4.set_title('Value vs Potential Analysis')

    # 5. Detailed Stats Table
    ax5 = fig.add_subplot(gs[2, :])
    ax5.axis('tight')
    ax5.axis('off')

    table_data = []
    for _, player in players_to_analyze.iterrows():
        table_data.append({
            'Player': player['short_name'],
            'Age': player['age'],
            'Club': player['club_name'],
            'Position': player['club_position'],
            'Overall': player['overall'],
            'Potential': player['potential'],
            'Sentiment Score': f"{player['normalized_sentiment']:.1f}",
            'Media Coverage': f"{player['coverage_score']:.1f}",
            'Value (M€)': f"{player['value_eur']/1000000:.1f}"
        })

    table = ax5.table(
        cellText=[[d[k] for k in table_data[0].keys()] for d in table_data],
        colLabels=table_data[0].keys(),
        loc='center',
        cellLoc='center'
    )
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1.2, 1.5)
    ax5.set_title('Detailed Player Statistics', pad=20)

    plt.tight_layout()
    plt.show()

def visualize_potential_distribution(results_df, min_age=17, max_age=23):
    """
    Create distribution visualizations for young players
    """
    young_players = results_df[
        (results_df['age'] >= min_age) &
        (results_df['age'] <= max_age)
    ]

    fig, axes = plt.subplots(2, 2, figsize=(15, 15))

    # 1. Potential Rating Distribution
    sns.histplot(
        data=young_players,
        x='potential_rating',
        bins=20,
        ax=axes[0, 0]
    )
    axes[0, 0].set_title('Distribution of Sentiment-based Potential Ratings')

    # 2. FIFA Potential vs Sentiment Potential
    sns.scatterplot(
        data=young_players,
        x='potential',
        y='potential_rating',
        ax=axes[0, 1]
    )
    axes[0, 1].set_title('FIFA Potential vs Sentiment-based Potential')

    # 3. Age vs Potential
    sns.boxplot(
        data=young_players,
        x='age',
        y='potential_rating',
        ax=axes[1, 0]
    )
    axes[1, 0].set_title('Potential Rating by Age')

    # 4. Position vs Potential
    sns.boxplot(
        data=young_players,
        x='club_position',
        y='potential_rating',
        ax=axes[1, 1]
    )
    axes[1, 1].set_title('Potential Rating by Position')
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

def main():
    # Read the results from sentiment analysis
    results_df = pd.read_csv('player_potential_analysis_final.csv')

    # Create visualizations for top 5 young players
    create_player_profile_visualization(results_df, top_n=5)

    # Create overall distribution visualizations
    visualize_potential_distribution(results_df)

    # Optionally analyze specific player
    # create_player_profile_visualization(results_df, player_name="Specific Player Name")

if __name__ == "__main__":
    main()